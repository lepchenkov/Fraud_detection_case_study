{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deploy ML models into production\n",
    "\n",
    "https://www.youtube.com/watch?v=-UYyyeYJAoQ\n",
    "Talk by Sumit Goyal, Software Engineer at IBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real value in DS project comes when you deploy the model on the web service. And deployment is rarely covered. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning workflow starts with business understanding, then followed by data understanding and data preparation. Then comes modelling, evaluation and deployment. \n",
    "\n",
    "But the work does not end with deployment.\n",
    "\n",
    "The data for the demo comes from https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment\n",
    "\n",
    "The really bad way to deal with the deployment is to create and train the model in python, then re-implement it in Java or C++ and then deploy into the Rest API. It is bad from time to value standpoint. \n",
    "\n",
    "There is also a PMML (Predictive Model Markup Language) but it is not very stable.\n",
    "\n",
    "The third way is to serialize the model and deploy it into the python web application serving a REST API. Serialized Object (a blob) is saved into database, and on top of the blob the database stores the version of the model, the name, type, ML Frameworks, Eval. metrics of the model, etc. And we need some sort of web framework, we load the model, create a route (like predict route), prepare the data, and then run the predict step and serve the prediction. In the ecommerce application when the transaction is created in the service, there is a call that is created to the ML endpoint of the REST API which asks is it a fraud or not. \n",
    "\n",
    "When the model is served intor production it is not a toy anymore. And there are service requirements: max response time, availability, quality/confidence of prediction, max re-train time, monitor. If the model is re-training, does it mean that we can't use the model anymore?\n",
    "\n",
    "#### Deployment with Cloud Foundry\n",
    "Cloud Foundry automates the deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
